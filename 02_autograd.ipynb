{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNW3k/RqpPgHMGlavogc8BB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://www.youtube.com/watch?v=DbeIqrwb_dE&list=PLqnslRFeH2UrcDBWF5mfPGpqQDSta6VK4&index=3\n","\n","https://github.com/patrickloeber/pytorchTutorial/blob/master/03_autograd.py"],"metadata":{"id":"4-E_xau9Kp8v"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"nzK5SgIuBN6s","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1714835818019,"user_tz":-120,"elapsed":4485,"user":{"displayName":"Nasreen Ahmed","userId":"04649825647281041989"}},"outputId":"f3dbdfd6-8239-4c0d-ed28-31b790095773"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([-0.0158,  0.1426, -0.1710], requires_grad=True)\n","tensor([1.9842, 2.1426, 1.8290], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7ba1612cd7b0>\n"]}],"source":["import torch\n","\n","# The autograd package provides automatic differentiation\n","# for all operations on Tensors\n","\n","# requires_grad = True -> tracks all operations on the tensor\n","x = torch.randn(3,requires_grad = True)\n","y = x + 2\n","\n","# y was created as a result of an operation, so it has a grad_fn attribute.\n","# grad_fn : references a Function that has created the Tensor\n","\n","print(x)\n","print(y)\n","print(y.grad_fn)"]},{"cell_type":"code","source":["# Do more operations on y\n","z = y * y * 3\n","print(z)\n","z = z.mean()\n","print(z)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nI5n44k7LjSj","executionInfo":{"status":"ok","timestamp":1714835857345,"user_tz":-120,"elapsed":546,"user":{"displayName":"Nasreen Ahmed","userId":"04649825647281041989"}},"outputId":"51102c59-d29f-465b-a5da-2816a2baec29"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([11.8117, 13.7720, 10.0360], grad_fn=<MulBackward0>)\n","tensor(11.8732, grad_fn=<MeanBackward0>)\n"]}]},{"cell_type":"code","source":["# Let's compute the gradients with backpropagation\n","# when we finish our computation we can call .backward() and have all the gradients computed automatically\n","# The gradient for this tensor will be accumulated into . grad() attribute\n","# It is the partial derivative of the function w.r.t the tensor\n","\n","z.backward()\n","print(x.grad) #dz/dx"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZE4bJ3gLrEB","executionInfo":{"status":"ok","timestamp":1714835945263,"user_tz":-120,"elapsed":231,"user":{"displayName":"Nasreen Ahmed","userId":"04649825647281041989"}},"outputId":"98074632-6706-4ab3-d2cd-60ef8eb73738"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([3.9685, 4.2852, 3.6581])\n"]}]},{"cell_type":"code","source":["# Stop a Tensor from tracking history:\n","# For example during our training loop when we want to update our weights\n","# then this update operation should not be part of the gradient computation\n","# - x.requires_grad(False)\n","# - x.detach()\n","# - wrap in 'with torch.no_grad()\n","\n","# requires_grad_() .. changes an existing flag in-place\n","\n","a = torch.randn(2,2)\n","print(a.requires_grad)\n","b = ((a * 3) / (a - 1))\n","print(b.grad_fn)\n","a.requires_grad_(True)\n","b = (a * a).sum()\n","print(b.grad_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zrrWs5jIMG3G","executionInfo":{"status":"ok","timestamp":1714836497949,"user_tz":-120,"elapsed":229,"user":{"displayName":"Nasreen Ahmed","userId":"04649825647281041989"}},"outputId":"f845b492-b455-4438-f764-690e0258cfa1"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n","None\n","<SumBackward0 object at 0x7ba1608e0e20>\n"]}]},{"cell_type":"code","source":["# .detach(): get a new Tensor with the same content but no gradient computations:\n","\n","a = torch.randn(2,2, requires_grad =True)\n","print(a.requires_grad)\n","b = a.detach()\n","print(b.requires_grad)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r2yhT8l7OOjy","executionInfo":{"status":"ok","timestamp":1714836593583,"user_tz":-120,"elapsed":221,"user":{"displayName":"Nasreen Ahmed","userId":"04649825647281041989"}},"outputId":"deda57e4-d2fe-433f-d09a-23c37a68537e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["True\n","False\n"]}]},{"cell_type":"code","source":["# -----------------------------------------------\n","# backward() accumulated the gradient for this tensor into .grad attribute\n","# !!! We need to be careful during optimization !!!\n","# Use .zero_() to empty the gradients before a new optimization step!\n","\n","weights = torch.ones(4,requires_grad=True)\n","\n","for epoch in range(3):\n","\n","  # just a dummy example\n","  model_output = ( weights * 3).sum()\n","  print(model_output)\n","  model_output.backward()\n","\n","  print(weights.grad)\n","\n","  #optimize model . i.e adjust weights\n","\n","  with torch.no_grad():\n","\n","    weights  -= 0.1 * weights.grad\n","\n","  # this is important ! It affects the final weights & Output\n","  weights.grad.zero_()\n","\n","print(weights)\n","print(model_output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PPKc9mknOfMv","executionInfo":{"status":"ok","timestamp":1714836954658,"user_tz":-120,"elapsed":229,"user":{"displayName":"Nasreen Ahmed","userId":"04649825647281041989"}},"outputId":"502a1266-ca2a-4e72-d1ca-48e2d9f8a7e1"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(12., grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","tensor(8.4000, grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","tensor(4.8000, grad_fn=<SumBackward0>)\n","tensor([3., 3., 3., 3.])\n","tensor([0.1000, 0.1000, 0.1000, 0.1000], requires_grad=True)\n","tensor(4.8000, grad_fn=<SumBackward0>)\n"]}]},{"cell_type":"code","source":["# Optimizer has zero_grad() method\n","# optimizer = troch.optim.SGD([weights],lr = 0.1)\n","# During training :\n","# optimizer.step()\n","# optimizer.zero_grad()"],"metadata":{"id":"7hSZkbYePmZz"},"execution_count":null,"outputs":[]}]}